# AI-learning

## Papers:

https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE

# AI PhD Preparation Guide

## Foundational Knowledge

- **Mathematics**: Linear algebra, calculus, probability, and statistics.
- **Programming**: Proficiency in languages like Python, and familiarity with libraries such as TensorFlow, PyTorch, and Keras.

## Machine Learning (ML)

- **Supervised Learning**: Regression, classification, and algorithms like decision trees, SVMs, and k-NN.
- **Unsupervised Learning**: Clustering (k-means, hierarchical), dimensionality reduction (PCA, t-SNE).
- **Reinforcement Learning**: Q-learning, deep Q-networks, policy gradients.
- **Model Evaluation and Selection**: Cross-validation, bias-variance tradeoff, performance metrics.

## Deep Learning (DL)

- **Neural Networks**: Basics of perceptrons, back-propagation, and activation functions.

- **Convolutional Neural Networks (CNNs):**  The Foundation of image deep learning.

- **Recurrent Neural Networks (RNNs)**: For sequence data, including LSTMs and GRUs.

- **Transformers**: Understanding attention mechanisms and their applications in NLP.

- **Generative Models**: GANs, VAEs.

## Natural Language Processing (NLP)

- **Text Preprocessing**: Tokenization, stemming, lemmatization.
- **Language Models**: BERT, GPT, and other transformer-based models.
- **Sequence-to-Sequence Models**: Machine translation, text summarization.
- **Information Retrieval and Extraction**: Named entity recognition, sentiment analysis.

## Computer Vision

- **Image Processing**: Filtering, edge detection, segmentation.
- **Object Detection**: YOLO, Faster R-CNN.
- **Image Generation**: Style transfer, deep dream.

## Data Engineering and Preprocessing

- **Data Cleaning**: Handling missing data, outliers.
- **Feature Engineering**: Creating meaningful features from raw data.
- **Data Augmentation**: Techniques to increase dataset diversity.

## Optimisation Techniques

- **Gradient Descent**: Variants like SGD, Adam, RMSprop.
- **Hyperparameter Tuning**: Grid search, random search, Bayesian optimisation.

## Writing GPU optimisation

- **Parallelism Techniques** (Data, Model, Pipeline)
- **GPU Architecture Understanding** (CUDA cores, Memory hierarchy)
- **Memory Optimisation** (Access patterns, Coalescing)
- **Kernel Optimisation** (Launch parameters, Thread divergence)
- **Mixed Precision Training** (FP16/FP32, Precision loss mitigation)
- **Batch Size Tuning** (Performance, Memory, Scaling)
- **Optimising Frameworks and Libraries** (TensorFlow, PyTorch, cuDNN)
- **Asynchronous Training** (Computation-communication overlap)
- **Distributed Training** (Horovod, TensorFlow Distributed)
- **Kernel Fusion and Reduction**
- **Profiling and Debugging Tools** (Nsight Systems, TensorBoard Profiler)
- **Quantisation and Pruning**
- **Dynamic Computational Graphs** (Eager execution, Static graphs)
- **System-level Optimisation** (CPU-GPU communication, PCIe bandwidth)
- **Automated Hyperparameter Tuning** (Bayesian optimization, Random search)

## Advanced Topics

- **Meta-Learning**: Learning to learn, few-shot learning.
- **Transfer Learning**: Applying knowledge from one domain to another.
- **Federated Learning**: Decentralized model training.
- **Multi-Agent Systems**: AI for systems involving multiple interacting agents.

## Practical Skills

- **Project Management**: Planning and executing research projects.
- **Collaboration Tools**: Git, collaborative platforms like Jupyter notebooks.
- **Communication**: Writing research papers, presenting findings.

## Staying Current

- **Reading Research Papers**: Regularly reading papers from top conferences (NeurIPS, ICML, CVPR).
- **Attending Conferences and Workshops**: Engaging with the AI research community.
- **Online Courses and Tutorials**: Continuously learning new skills and techniques.
